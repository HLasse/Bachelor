---
title: "Bachelor"
author: "Lasse Hansen"
date: "October 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("~/Desktop/test/2/sliced")
p_load(tidyverse, stringr)

```


##Function to load and add a column for filename
```{r}
fileload <- function(file){
  df <- read_csv(file)
#  df = df[!df$NAQ==0,]
  df$filename <- as.factor(file)
  return(df)
}
```

#loading data
```{r}
file.list <- list.files( pattern = '*.csv', full.names = T)


df.list <- lapply(file.list, fileload)
#turning into a data frame
df <- bind_rows(df.list)

```

##Preprocessing
```{r Preprocessing}
#defining voice activity as VAD being > 0.6. Adding binary column with voice 1 and no voice 0 
df$voice <- ifelse(df$VAD > 0.6, 1, 0)
#Since each file is split in several files, we create a single id column, containing only the original file name
df$id <- substr(df$filename, 3, 9) %>% 
  str_remove_all("_")


# 30 seconds = 3000 rows (1 row = 10 ms )
#adding 30 second groups (in column called .groups)
df <- df %>%
  group_by(id) %>%
  #creates a group column containing 3000 rows (ie. 30 secs)
  #if there are not enough data points to fill up a group, the remaining data is discarded
  do(groupdata2::group(., n=  3000, method = "greedy", force_equal = T)) %>% 
  # Convert .subgroups to an integer and then to a factor
  mutate(.groups = as.integer(.groups),
         .groups = as.factor(.groups))

##########################  PAUSES ###################################

#creating df with only the pauses. A column called consec is added which counts the number of consecutive value in voice.
#data points with no voice activity are discarded
pauseDf <- df %>% 
  select(time, id, voice, .groups) %>%  
  group_by(id, .groups) %>%
  mutate(consec = sequence(rle(as.character(voice))$lengths)) %>%
  filter(voice == 0 )

#finding the beginning of a new pause (discarding the first value since its row 1)
flag_one <- which(pauseDf$consec == 1)[-1]
#substracting 1 to find the pause length
one_before <- flag_one - 1
#creating df with the pause lengths 
pauseDf <- pauseDf[one_before,]
#if there is less than 250 ms or more than 10 seconds before next voice activity it is not considered a pause
pauseDf <- pauseDf %>%
  filter(consec > 25 & consec < 1000)

#summarising pauses to mean length, sd of length and number of pauses
sumPauseDf <- pauseDf %>% 
  group_by(id, .groups) %>% 
  summarise(meanPauseLength = mean(consec), medianPauseLength = median(consec), sdPauseLength = sd(consec), iqrPauseLength = IQR(consec), nPauses = n(), totalPauseTime = sum(consec))

##########################  SUMMARISING FEATURES ###################################

#removing features which could not be extracted
remove <- c("HMPDM_0", "HMPDM_1", "HMPDM_2", "vowelSpace")
df[, remove] <- NULL

#removing columns without voice activity and summarising with median and iqr (sd, others???)
sumdf <- df %>% 
  filter(voice == 1) %>%
  group_by(id, .groups) %>% 
  summarise_at(.vars = names(.)[2:78],
               .funs = c(median="median", IQR = "IQR"))

#merging pauses and features
mergedDf <- merge(sumPauseDf, sumdf, by = c("id", ".groups"))

#creating new column to merge clinical features
mergedDf$mergeID <- strsplit(mergedDf$id, "v")[[1]][1]

####MAKE NEW COLUMN IN SUM DF WITH ID WITH NO V1/V2


#loading and merges with clinical and demographical data
demoData <- read.csv("/home/lasse/Desktop/Bachelor/DemClinicalData.csv", sep = ";")
#removing the chronic depression patients
demoData <- demoData %>%
  filter(Diagnosis2 != "ChronicDepression")
#creating new column
demoData$temp <- ifelse(demoData$Diagnosis == "Control", "dc", "dp")
demoData$mergeID <- paste0(demoData$temp, demoData$ID)

#should be mergeable now by mergeID


#subsetting
depDf <- subset(mergedDf, diagnosis == 1)

#subset only for controls and dp_v1

# Split in test and training sets (use groupdata2)

# Write to files
write.csv(depDf, "depDf.csv")
write.csv(contDep, "contDep.csv")
```


Preparing the elastic net
```{r elastic net}

#################### MODEL 1 - DIAGNOSIS FROM VOICE ###############
contDep < read.csv("contDep.csv")- 
#df$X <- NULL


#creating predictor column 
xContDep <- contDep[, -c(158:168)] #change to the values in the set
xContDep$filename = NULL

#creating outcome variable
yContDep <- select(contDep, diagnosis) 

#check for NA
#identical(xContDep, xContDep[complete.cases(xContDep),])

#################### MODEL 2 - REMISSION FROM VOICE ###############
dep <- read.csv('depDf.csv')
#df$X <- NULL


#creating predictor column 
xDep <- dep[, -c(158:168)] #change to the values in the set
xDep$filename = NULL

#creating outcome variable
yDep <- select(contDep, remission) 

#check for NA
#identical(xDep, xDep[complete.cases(xDep),])

```

##Running the elastic net
```{r}
## Define predictors and outcome
xContDep <- model.matrix(~.-1, data= xContDep) # where all the columns except the first are predictors
xDep <- model.matrix(~.-1, data= xDep)


# makes outcome variables into a non data frame because glmnet doesnâ€™t deal with dataframes
yContDep <- as.numeric(unlist(yContDep))
yDep <- as.numeric(unlist(yDep))


#scale all features
xContDep <- scale(xContDep)[,]
xDep <- scale(xDep)[,]

#function to run the elastic net. First chooses the optimal alpha, then runs the elastic net
elasticfun <- function(y, x){
  # Define cross-validated alpha selection (alpha indicates how strong the correlation btw predictors should be for it to be taken   into account when discarding predictors)
  alphaslist<-seq(0,1,by=0.1)
  foldslist<-seq(4,12)
  pars=expand.grid(alphaslist,foldslist)

  cvm1=matrix(rep(0,length(alphaslist)))

  elasticnet1<-lapply(1:length(cvm1), 
                  function(a){
                    cv.glmnet(x, y, alpha=alphaslist[a], family="binomial", 
                                                         lambda.min.ratio=.001,nfolds = 5)})

  for (i in 1:length(alphaslist)) {cvm1[i]=min(elasticnet1[[i]]$cvm)}

  n1=which(cvm1==min(cvm1))

  alpha1=alphaslist[n1]

# Run cross-validated elastic net with the chosen alpha (to choose lambda)
  mod_cv <- cv.glmnet(x=x, y=y, family='binomial', alpha=alpha1, nfolds=5) # Modify family if not binomial
  coefs=as.data.frame(as.matrix(coef(mod_cv, mod_cv$lambda.1se)))
  coefs$predictors<-rownames(coefs)
  rownames(coefs) <- NULL
  names(coefs)[1] <- "betas"
  coefs=subset(coefs,betas!=0)
  coefs1=coefs[order(abs(coefs$betas)),]
  return(coefs1)
}


contDepNet <- elasticfun(yContDep, xContDep)
depNet <- elasticfun(yDep, xDep)
# #saving values
# save(contDepNet, file = "contDepNet.RData")
# save(depNet, file = "depNet.RData")
```




Bayesian logistic models
```{r}

```

SVM models
```{r}
# remember to tune and optimize https://www.r-bloggers.com/machine-learning-using-support-vector-machines/
# non linear svm? https://machinelearningmastery.com/non-linear-regression-in-r/ 
#                 https://stats.stackexchange.com/questions/10551/how-do-i-choose-what-svm-kernels-to-use

```

Random forest (adaboost or xgboost?)
```{r}

```

Naive Bayes
```{r}

```

Nearest neighbors
```{r}

```

Ensemble
```{r}

```



